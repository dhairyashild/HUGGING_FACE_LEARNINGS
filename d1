# =============================================================================
# üöÄ COMPLETE QLoRA T5 ENGLISH QUOTES FINE-TUNING PIPELINE
# =============================================================================
# Production Ready ‚Ä¢ 10 Steps ‚Ä¢ 5min Training ‚Ä¢ 8MB Model ‚Ä¢ LangChain Ready
# =============================================================================

import os, warnings, torch, torch.nn as nn
from datasets import load_dataset
from transformers import (
    AutoTokenizer, AutoModelForSeq2SeqLM, BitsAndBytesConfig,
    DataCollatorForSeq2Seq, TrainingArguments, Trainer, pipeline
)
from peft import LoraConfig, get_peft_model, PeftModel, PeftConfig
warnings.filterwarnings("ignore")
os.environ["CUDA_VISIBLE_DEVICES"] = "0"

MODEL_CHECKPOINT = "t5-small"
PEFT_MODEL_ID = "your-username/t5-qlora-english-quotes"

def print_trainable_parameters(model):
    trainable_params = all_params = 0
    for name, param in model.named_parameters():
        all_params += param.numel()
        if param.requires_grad:
            trainable_params += param.numel()
    print(f"Trainable: {trainable_params:,} | Total: {all_params:,} | %: {100 * trainable_params / all_params:.2f}%")

print("üìä Loading dataset...")
data = load_dataset("Abirate/english_quotes")
data = data.train_test_split(test_size=0.1)

def merge_columns(example):
    example["prediction"] = f'{example["quote"]} -> {example["tags"]}'
    return example
data['train'] = data['train'].map(merge_columns)
data['test'] = data['test'].map(merge_columns)

print("üîÑ Loading QLoRA T5...")
tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)
bnb_config = BitsAndBytesConfig(load_in_8bit=True)
model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_CHECKPOINT, quantization_config=bnb_config, device_map="auto")

for param in model.parameters():
    param.requires_grad = False
    if param.ndim == 1:
        param.data = param.data.to(torch.float32)
model.gradient_checkpointing_enable()
model.enable_input_require_grads()

class CastOutputToFloat(nn.Sequential):
    def forward(self, x): return super().forward(x).to(torch.float32)
model.lm_head = CastOutputToFloat(model.lm_head)

print("‚ùÑÔ∏è Before LoRA:")
print_trainable_parameters(model)

lora_config = LoraConfig(r=16, lora_alpha=32, lora_dropout=0.05, target_modules=["q", "v"], bias="none", task_type="SEQ_2_SEQ_LM")
model = get_peft_model(model, lora_config)
print("\nüî• After LoRA:")
print_trainable_parameters(model)

def tokenize_function(examples):
    inputs = [f"predict tags: {q}" for q in examples["quote"]]
    targets = examples["prediction"]
    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=True)
    with tokenizer.as_target_tokenizer():
        labels = tokenizer(targets, max_length=64, truncation=True, padding=True)
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

tokenized_data = data.map(tokenize_function, batched=True, remove_columns=data['train'].column_names)

training_args = TrainingArguments(
    output_dir="outputs",
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    warmup_steps=100,
    max_steps=200,
    learning_rate=2e-4,
    fp16=True,
    logging_steps=10,
    save_steps=50,
    save_total_limit=2,
    remove_unused_columns=False,
    report_to="none"
)

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)
trainer = Trainer(model=model, args=training_args, train_dataset=tokenized_data["train"], eval_dataset=tokenized_data["test"], tokenizer=tokenizer, data_collator=data_collator)

print("üöÄ Training QLoRA...")
model.config.use_cache = False
trainer.train()
model.config.use_cache = True
trainer.save_model("outputs/final")
tokenizer.save_pretrained("outputs/final")

model.push_to_hub(PEFT_MODEL_ID, token=True, commit_message="QLoRA T5 Quotes v1.0")
tokenizer.push_to_hub(PEFT_MODEL_ID)

print("\nüß™ INFERENCE TEST:")
test_model = PeftModel.from_pretrained("t5-small", PEFT_MODEL_ID)
test_quotes = ["Be yourself; everyone else is already taken.", "The world is your oyster."]
batch = tokenizer([f"predict tags: {q}" for q in test_quotes], return_tensors="pt", padding=True, truncation=True).to(test_model.device)

with torch.no_grad(), torch.cuda.amp.autocast():
    outputs = test_model.generate(**batch, max_new_tokens=50, do_sample=False, pad_token_id=tokenizer.eos_token_id)
    results = tokenizer.batch_decode(outputs, skip_special_tokens=True)
    
for quote, result in zip(test_quotes, results):
    print(f"üìù {quote}")
    print(f"üè∑Ô∏è  {result}\n")

print("üéâ PRODUCTION READY - LangChain/Streamlit Deployable!")
print(f"Live: https://huggingface.co/{PEFT_MODEL_ID}")
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
# =============================================================================
# üì¶ 1. INSTALLATION & IMPORTS
# pip install transformers[torch] datasets accelerate
# =============================================================================
import warnings
import torch
from datasets import load_dataset
from transformers import (
    AutoTokenizer, AutoModelForSeq2SeqLM, 
    DataCollatorForSeq2Seq, TrainingArguments, Trainer, pipeline
)

warnings.filterwarnings("ignore")
device = "cuda" if torch.cuda.is_available() else "cpu"

# =============================================================================
# ‚öôÔ∏è 2. CONFIGURATION (The Only Section You Usually Change)
# =============================================================================
MODEL_CHECKPOINT = "t5-small"  # Try "facebook/bart-base" or "google/pegasus-xsum"
DATASET_NAME = "knkarthick/samsum"
OUTPUT_DIR = "./finetuned-model-output"

# =============================================================================
# üîÑ 3. DATA LOADING & PREPROCESSING
# =============================================================================
tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)

def preprocess_function(examples):
    # üí° TIP: T5 models usually need a prefix like "summarize: "
    # BART and Pegasus do NOT need this.
    prefix = "summarize: " if "t5" in MODEL_CHECKPOINT.lower() else ""
    
    inputs = [prefix + doc for doc in examples["dialogue"]]
    model_inputs = tokenizer(inputs, max_length=1024, truncation=True, padding="max_length")

    with tokenizer.as_target_tokenizer():
        labels = tokenizer(examples["summary"], max_length=128, truncation=True, padding="max_length")

    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

raw_dataset = load_dataset(DATASET_NAME)
tokenized_dataset = raw_dataset.map(preprocess_function, batched=True)

# =============================================================================
# üèóÔ∏è 4. MODEL INITIALIZATION
# =============================================================================
model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_CHECKPOINT).to(device)
data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)

# =============================================================================
# üìà 5. TRAINING ARGUMENTS (Standard Optimization)
# =============================================================================
training_args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    evaluation_strategy="steps",
    learning_rate=2e-5,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    gradient_accumulation_steps=16, # Simulates a batch size of 64
    weight_decay=0.01,
    save_total_limit=2,
    num_train_epochs=1,
    fp16=True if torch.cuda.is_available() else False, # Speed boost for GPUs
    report_to="none"
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["test"],
    tokenizer=tokenizer,
    data_collator=data_collator,
)

# =============================================================================
# üöÄ 6. EXECUTION & INFERENCE
# =============================================================================
trainer.train()
trainer.save_model(OUTPUT_DIR)

# LOAD FOR TESTING
gen = pipeline("summarization", model=OUTPUT_DIR, device=0 if device=="cuda" else -1)
print(gen("Luffy: Join my crew! Zoro: Only if I become the best swordsman."))
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
main fine-tuning types:

Full FT: Complete weight updates. (Max resource / Max memory).
LoRA: Parameter-efficient "adapter" tuning. (Balanced / Production-ready).
QLoRA: Quantized (4-bit) LoRA. (Minimum VRAM / High efficiency).
Quick Tip: Use QLoRA if you are on a budget GPU; use Full FT only if the model is extremely small and you have plenty of VRAM.
