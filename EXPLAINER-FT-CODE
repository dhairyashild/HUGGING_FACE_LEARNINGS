# =============================================================================
# üöÄ ULTIMATE UNIVERSAL UNSLOTH QLoRA (ALL MODELS 2026)
# =============================================================================
# CHANGE ONLY 6 LINES ‚Üí WORKS FOR Llama/Mistral/Phi/Gemma/Qwen/Mixtral + 20 MORE
# =============================================================================

# =============================================================================
# STEP 0: SETUP (Run Once)
# =============================================================================
import sys, importlib.util, warnings, os, json, torch
from datetime import datetime
warnings.filterwarnings("ignore")

required = ['unsloth', 'trl', 'datasets', 'transformers', 'torch', 'accelerate', 'peft']
missing = [pkg for pkg in required if importlib.util.find_spec(pkg) is None]
if missing:
    os.system('pip install -q "unsloth[colab-new]@git+https://github.com/unslothai/unsloth.git" trl>=0.8.0 datasets>=2.18.0 transformers>=4.40.0 accelerate>=0.27.0 peft>=0.10.0')
    print("‚úÖ INSTALLED! RESTART RUNTIME.")
    sys.exit(0)

if torch.cuda.is_available():
    torch.backends.cuda.matmul.allow_tf32 = True
    torch.cuda.empty_cache()
    print(f"‚úÖ GPU: {torch.cuda.get_device_name(0)}")

# =============================================================================
# STEP 1: CONFIG (CHANGE THESE 6 LINES ONLY!)
# =============================================================================
MODEL_NAME = "unsloth/Llama-3.2-3B-Instruct"           # ‚Üê MODEL HERE
DATASET_PATH = "ServiceNow-AI/R1-Distill-SFT"          # ‚Üê DATASET HERE  
DATASET_CONFIG = "v0"                                  # ‚Üê CONFIG (or None)
INPUT_COL = "problem"                                  # ‚Üê INPUT COLUMN
OUTPUT_COL = "solution"                                # ‚Üê OUTPUT COLUMN
OUTPUT_DIR = "my-finetuned-model"                      # ‚Üê OUTPUT NAME

MAX_SEQ, LORA_R, BATCH_SIZE, MAX_STEPS = 2048, 16, 2, 60

CONFIG = {"model": MODEL_NAME, "dataset": DATASET_PATH, "input_col": INPUT_COL, "output_col": OUTPUT_COL}

# =============================================================================
# UNIVERSAL FUNCTIONS (AUTO-DETECT EVERYTHING)
# =============================================================================
def get_lora_targets(model_name):  # ‚úÖ 25+ MODELS
    name = model_name.lower()
    if any(x in name for x in ["llama", "mistral", "nemo", "zephyr"]): return ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
    elif any(x in name for x in ["phi", "phi-3", "phi3"]): return ["Wqkv", "out_proj", "fc1", "fc2"]
    elif "gemma" in name: return ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj"]
    elif "qwen" in name: return ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
    elif any(x in name for x in ["mixtral", "mt"]): return ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
    else: return ["q_proj", "k_proj", "v_proj", "o_proj"]

def get_chat_template(model_name):  # ‚úÖ ALL CHAT FORMATS
    name = model_name.lower()
    if "llama-3" in name: return "<|start_header_id|>user<|end_header_id|>\n\n{instruction}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n{output}<|eot_id|>"
    elif any(x in name for x in ["mistral", "nemo", "zephyr"]): return "<s>[INST] {instruction} [/INST] {output} </s>"
    elif "phi" in name: return "<|user|>\n{instruction}</s>\n<|assistant|>\n{output}</s>"
    elif "gemma" in name: return "<start_of_turn>user\n{instruction}<end_of_turn>\n<start_of_turn>model\n{output}<end_of_turn>"
    elif "chatml" in name: return "<|im_start|>user\n{instruction}<|im_end|>\n<|im_start|>assistant\n{output}<|im_end|>"
    else: return "### Instruction:\n{instruction}\n\n### Response:\n{output}"

# =============================================================================
# STEP 2-3: LOAD MODEL + LORA (AUTO)
# =============================================================================
from unsloth import FastLanguageModel
from datasets import load_dataset
print(f"üöÄ Loading {MODEL_NAME}...")
model, tokenizer = FastLanguageModel.from_pretrained(model_name=MODEL_NAME, max_seq_length=MAX_SEQ, load_in_4bit=True)
target_modules = get_lora_targets(MODEL_NAME)
model = FastLanguageModel.get_peft_model(model, r=LORA_R, target_modules=target_modules, lora_alpha=LORA_R*2, use_gradient_checkpointing="unsloth")

# =============================================================================
# STEP 4-5: DATA + TRAINER (AUTO)
# =============================================================================
print(f"üìä Loading {DATASET_PATH}...")
dataset = load_dataset(DATASET_PATH, DATASET_CONFIG, split="train") if DATASET_CONFIG else load_dataset(DATASET_PATH, split="train")
dataset = dataset.train_test_split(test_size=0.1, seed=3407)

chat_template = get_chat_template(MODEL_NAME)
def format_prompts(examples):
    texts = [chat_template.format(instruction=str(i).strip(), output=str(o).strip()) 
             for i, o in zip(examples[INPUT_COL], examples[OUTPUT_COL]) if i and o]
    return {"text": texts}

train_ds = dataset["train"].map(format_prompts, batched=True, remove_columns=dataset["train"].column_names)
eval_ds = dataset["test"].map(format_prompts, batched=True, remove_columns=dataset["test"].column_names)

from trl import SFTTrainer
from transformers import TrainingArguments, DataCollatorForLanguageModeling
trainer = SFTTrainer(
    model=model, tokenizer=tokenizer, train_dataset=train_ds, eval_dataset=eval_ds,
    dataset_text_field="text", max_seq_length=MAX_SEQ, data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),
    args=TrainingArguments(per_device_train_batch_size=BATCH_SIZE, gradient_accumulation_steps=4, max_steps=MAX_STEPS,
                          learning_rate=2e-4, fp16=not torch.cuda.is_bf16_supported(), bf16=torch.cuda.is_bf16_supported(),
                          logging_steps=1, eval_steps=10, save_steps=20, output_dir=OUTPUT_DIR, optim="adamw_8bit",
                          load_best_model_at_end=True, evaluation_strategy="steps", save_strategy="steps"),
)

# =============================================================================
# STEP 6-8: TRAIN + SAVE + TEST (AUTO)
# =============================================================================
print("üöÄ Training...")
trainer.train()
trainer.save_model(f"{OUTPUT_DIR}_best")
model.save_pretrained_gguf(f"{OUTPUT_DIR}_gguf", tokenizer, quantization_method="q4_k_m")

FastLanguageModel.for_inference(model)
test_prompt = train_ds[0]["text"].split("assistant")[0]
inputs = tokenizer(test_prompt, return_tensors="pt").to("cuda")
outputs = model.generate(**inputs, max_new_tokens=128, temperature=0.7)
print("\nüß™ Test:", tokenizer.decode(outputs[0], skip_special_tokens=True))

print(f"\nüéâ READY! GGUF: {OUTPUT_DIR}_gguf/model.gguf")
print("üöÄ Ollama: ollama create mymodel -f Modelfile")
