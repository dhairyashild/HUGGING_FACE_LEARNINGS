# STEP 0: DEPENDENCY VALIDATION & GPU OPTIMIZATION (Run Once)
# =============================================================================
import sys, importlib.util, warnings, os, json, torch
from datetime import datetime
warnings.filterwarnings("ignore")

# Install missing packages FIRST
required = ['unsloth', 'trl', 'datasets', 'transformers', 'torch', 'accelerate', 'peft']
missing = [pkg for pkg in required if importlib.util.find_spec(pkg) is None]

if missing:
    print("‚ùå MISSING:", " ".join(missing))
    print("\nüì¶ INSTALLING NOW...")
    os.system('pip install -q "unsloth[colab-new]@git+https://github.com/unslothai/unsloth.git" trl>=0.8.0 datasets>=2.18.0 transformers>=4.40.0 accelerate>=0.27.0 peft>=0.10.0')
    print("‚úÖ INSTALLED! RESTART RUNTIME NOW.")
    sys.exit(0)

# GPU optimization
if torch.cuda.is_available():
    torch.backends.cuda.matmul.allow_tf32 = True
    torch.cuda.empty_cache()
    print(f"‚úÖ GPU READY: {torch.cuda.get_device_name(0)} ({torch.cuda.get_device_properties(0).total_memory/1e9:.1f}GB)")
else:
    print("‚ö†Ô∏è  No GPU - use Colab T4/A100")

print("üöÄ STEP 0 COMPLETE ‚Üí PROCEED TO STEP 1!")


######################################################################
# =============================================================================
# STEP 1: PROJECT CONFIGURATION (CHANGE THESE 6 LINES ONLY)
# =============================================================================
MODEL_NAME = "unsloth/Llama-3.2-3B-Instruct"      # # WHICH AI MODEL to download/train
DATASET_PATH = "ServiceNow-AI/R1-Distill-SFT"     # # YOUR TRAINING DATA location
DATASET_CONFIG = "v0"                             # # DATASET version/subset to use
INPUT_COL = "problem"                             # # COLUMN with questions/prompts
OUTPUT_COL = "solution"                           # # COLUMN with answers/responses
OUTPUT_DIR = "my-finetuned-llama"                 # # FOLDER NAME for your trained model

MAX_SEQ = 2048                                    # # Max words model remembers (2048=~1500 words)
LORA_R = 16                                       # # Fine-tuning "strength" (16=good balance)
BATCH_SIZE = 2                                    # # How many examples per GPU batch (2=safe)
MAX_STEPS = 60                                    # # Training rounds (60=quick test, 1000=full)

# # SAVE ALL SETTINGS in 1 place (for logging/export)
CONFIG = {
    "model": MODEL_NAME, "dataset": DATASET_PATH, "input_col": INPUT_COL,
    "output_col": OUTPUT_COL, "max_seq": MAX_SEQ, "lora_r": LORA_R,
    "batch_size": BATCH_SIZE, "max_steps": MAX_STEPS, "created": datetime.now().isoformat()
}

# =============================================================================
# STEP 2: LOAD MODEL & TOKENIZER
# =============================================================================
from unsloth import FastLanguageModel                    # # Import Unsloth's super-fast model loader (2x faster than normal)

print(f"üöÄ Loading {MODEL_NAME}...")                      # # Print progress: "Loading unsloth/Llama-3.2-3B-Instruct..."

# # DOWNLOAD + LOAD AI BRAIN + TEXT CONVERTER from HuggingFace
model, tokenizer = FastLanguageModel.from_pretrained(     # # Returns 2 things: model (AI brain) + tokenizer (text‚Üínumbers)
    model_name=MODEL_NAME,                                # # Which model: "unsloth/Llama-3.2-3B-Instruct" (from STEP 1)
    max_seq_length=MAX_SEQ,                               # # Max conversation length: 2048 tokens (~1500 words)
    dtype=None,                                           # # Auto-select best math type for your GPU (bf16/fp16)
    load_in_4bit=True,                                    # # CRUCIAL: Compress model 4x smaller (3B‚Üí1.5GB RAM usage)
)                                                        # # Downloads ‚Üí Compresses ‚Üí Loads into GPU memory = READY!


# =============================================================================
# STEP 3: UNIVERSAL LORA CONFIGURATION
# =============================================================================

def get_lora_targets(model_name):                                # # SMART FUNCTION: Picks correct LoRA layers for ANY model
    name = model_name.lower()                                    # # Convert "Llama-3.2" ‚Üí "llama-3.2" (lowercase)
    if any(x in name for x in ["llama", "mistral"]):             # # IF Llama/Mistral ‚Üí use 7 attention layers
        return ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
    elif "phi" in name: return ["Wqkv", "out_proj", "fc1", "fc2"]# # IF Phi ‚Üí use Phi layers
    elif "gemma" in name: return ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj"]
    else: return ["q_proj", "k_proj", "v_proj", "o_proj"]         # # DEFAULT: Basic 4 attention layers

target_modules = get_lora_targets(MODEL_NAME)                    # # AUTO-PICK: ["q_proj", "k_proj", ...] for Llama-3.2
print(f"üéØ LoRA targets: {target_modules}")                      # # Show: "q_proj k_proj v_proj..." (7 layers)

# # ADD LoRA "STICKERS" to model (train ONLY 1% of weights)
model = FastLanguageModel.get_peft_model(                        # # WRAP model with LoRA adapters
    model,                                                       # # Input: Base model from STEP 2
    r=LORA_R,                                                    # # Rank=16: LoRA "thickness" (higher=more power)
    target_modules=target_modules,                               # # WHICH 7 layers to modify (q_proj, k_proj...)
    lora_alpha=LORA_R*2,                                         # # Scaling=32: Makes LoRA stronger (r*2 rule)
    lora_dropout=0,                                              # # Dropout=0%: No randomness (faster training)
    bias="none",                                                 # # No bias training (saves memory)
    use_gradient_checkpointing="unsloth",                        # # Unsloth's memory trick (2x less RAM)
    random_state=3407,                                           # # Fixed seed (same results every run)
)

# # RESULT: 3B model ‚Üí ONLY 4.5M trainable params (0.15% of total!)


# =============================================================================
# STEP 4: LOAD & SPLIT DATASET
# =============================================================================
from datasets import load_dataset                                    # # Import HuggingFace dataset loader

print(f"üìä Loading {DATASET_PATH}...")                                # # Show: "Loading ServiceNow-AI/R1-Distill-SFT..."

try:                                                                 # # TRY best method first
    dataset = load_dataset(DATASET_PATH, DATASET_CONFIG, split="train") if DATASET_CONFIG else load_dataset(DATASET_PATH, split="train")
    # # Method 1: Full dataset + config ("ServiceNow-AI/R1-Distill-SFT", "v0", "train")
except:                                                              # # CATCH any error
    dataset = load_dataset(DATASET_PATH, split="train[:1000]")       # # FALLBACK: First 1000 examples only

dataset = dataset.train_test_split(test_size=0.1, seed=3407, shuffle=True)  # # SPLIT: 90% train | 10% validation
print(f"‚úÖ Train: {len(dataset['train'])} | Eval: {len(dataset['test'])}")  # # Show: "Train: 9000 | Eval: 1000"


# =============================================================================
# STEP 5: UNIVERSAL DATA FORMATTING
# =============================================================================
def get_chat_template(model_name):                                    # # SMART: Picks correct chat format for ANY model
    name = model_name.lower()                                         # # "Llama-3.2-3B" ‚Üí "llama-3.2-3b"
    if "llama-3" in name:                                             # # Llama-3.2 NEEDS special tags
        return "<|start_header_id|>user<|end_header_id|>\n\n{instruction}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n{output}<|eot_id|>"
    elif any(x in name for x in ["mistral", "zephyr"]):                # # Mistral format
        return "<|user|>\n{instruction}</s>\n<|assistant|>\n{output}</s>"
    elif "chatml" in name:                                             # # OpenAI format
        return "<|im_start|>user\n{instruction}<|im_end|>\n<|im_start|>assistant\n{output}<|im_end|>"
    else:                                                              # # Simple fallback
        return "### Instruction:\n{instruction}\n\n### Response:\n{output}"

chat_template = get_chat_template(MODEL_NAME)                         # # AUTO-PICK: Llama-3 format for Llama-3.2

def format_prompts(examples):                                         # # Convert raw data ‚Üí chat format
    instructions = examples[INPUT_COL]                                # # Grab "problem" column
    outputs = examples[OUTPUT_COL]                                    # # Grab "solution" column
    texts = []
    for i, o in zip(instructions, outputs):                           # # Loop each pair
        if i and o and str(i).strip() and str(o).strip():             # # Skip empty rows
            texts.append(chat_template.format(instruction=str(i).strip(), output=str(o).strip()))
    return {"text": texts}                                            # # Return 1 column: formatted chats

# # APPLY formatting to train/eval datasets
train_ds = dataset["train"].map(format_prompts, batched=True, remove_columns=dataset["train"].column_names)
eval_ds = dataset["test"].map(format_prompts, batched=True, remove_columns=dataset["test"].column_names)
# # Result: Raw data ‚Üí "text" column with perfect chat format

# =============================================================================
# STEP 6: PRODUCTION SFT TRAINER
# =============================================================================
from trl import SFTTrainer                                            # # Supervised Fine-Tuning trainer
from transformers import TrainingArguments, DataCollatorForLanguageModeling  # # Training settings

trainer = SFTTrainer(                                                 # # CREATE TRAINER (all-in-one)
    model=model, tokenizer=tokenizer, train_dataset=train_ds, eval_dataset=eval_ds,  # # Model + formatted data
    dataset_text_field="text", max_seq_length=MAX_SEQ,                # # Use "text" column, cut to 2048 tokens
    
    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),  # # Auto-tokenize + pad batches
    
    args=TrainingArguments(                                           # # DETAILED TRAINING SETTINGS
        per_device_train_batch_size=BATCH_SIZE, per_device_eval_batch_size=BATCH_SIZE,  # # 2 examples per batch
        gradient_accumulation_steps=4, warmup_ratio=0.1, max_steps=MAX_STEPS,           # # Effective batch=8, train 60 steps
        learning_rate=2e-4,                                           # # Learning speed (0.0002)
        fp16=not torch.cuda.is_bf16_supported(), bf16=torch.cuda.is_bf16_supported(),   # # Auto-pick GPU math (fp16/bf16)
        
        logging_steps=1, eval_steps=10, save_steps=20, evaluation_strategy="steps", save_strategy="steps",  # # Log every step, eval every 10
        load_best_model_at_end=True, metric_for_best_model="eval_loss", greater_is_better=False,             # # Keep BEST model
        
        output_dir=OUTPUT_DIR, report_to="none", optim="adamw_8bit", lr_scheduler_type="cosine",  # # Save to folder, no wandb
        seed=3407, gradient_checkpointing=True, remove_unused_columns=False,  # # Fixed results, save RAM
    ),
)


# =============================================================================
# STEP 7: TRAIN & SAVE (FINAL STEP!)
# =============================================================================
print("üöÄ Training...")                                   # # START TRAINING (60 steps = ~5-10 mins)

trainer.train()                                          # # MAGIC: Trains LoRA adapters on your data
print("‚úÖ Training complete!")                            # # DONE! Model learned your dataset

# # SAVE 3 FORMATS (production ready)
trainer.save_model(f"{OUTPUT_DIR}_best")                 # # 1Ô∏è‚É£ FULL MODEL (merged LoRA + base)
model.save_pretrained(f"{OUTPUT_DIR}_lora")              # # 2Ô∏è‚É£ LoRA ONLY (small 50MB adapter)
model.save_pretrained_gguf(f"{OUTPUT_DIR}_gguf", tokenizer, quantization_method="q4_k_m")  # # 3Ô∏è‚É£ GGUF (Ollama/LocalAI ready, 4-bit)

# # SAVE PROJECT CONFIG
CONFIG["status"] = "completed"                           # # Mark as done
with open(f"{OUTPUT_DIR}_config.json", "w") as f:        # # Save all settings
    json.dump(CONFIG, f, indent=2)

print(f"\nüéâ PRODUCTION READY!")                          # # CELEBRATE!
print(f"üìÅ Full: {OUTPUT_DIR}_best/")                    # # Use for HF Inference
print(f"üîß LoRA: {OUTPUT_DIR}_lora/")                    # # Use to merge later
print(f"ü¶ô GGUF: {OUTPUT_DIR}_gguf/model.gguf")          # # Use with Ollama/LocalAI
print(f"üìÑ Config: {OUTPUT_DIR}_config.json")            # # Re-run same project


# =============================================================================
# STEP 8: INFERENCE TEST (Try your trained model!)
# =============================================================================
FastLanguageModel.for_inference(model)                           # # SWITCH model to FAST GENERATION mode

# # Grab 1st training example (question only, no answer)
test_prompt = train_ds[0]["text"].split("<|start_header_id|>assistant<|end_header_id|>")[0]
# # Result: "<|start_header_id|>user<|end_header_id|>\n\nHow to fix server error?"

inputs = tokenizer(test_prompt, return_tensors="pt").to("cuda" if torch.cuda.is_available() else "cpu")
# # Convert text ‚Üí numbers ‚Üí send to GPU

outputs = model.generate(                                        # # GENERATE NEW ANSWER!
    **inputs, max_new_tokens=128, temperature=0.7                # # 128 new words, 0.7=creative but focused
)
print("\nüß™ Test:", tokenizer.decode(outputs[0], skip_special_tokens=True))
# # Show: "Your trained model says: Fix by restarting nginx service..."

print("\nüöÄ DEPLOY ANYWHERE:")                                   # # HOW TO USE YOUR MODEL
print("‚Ä¢ HF: from_pretrained('your-hf-repo')")                   # # HuggingFace inference
print("‚Ä¢ Ollama: ollama create mymodel -f Modelfile")            # # Local Ollama server
print("‚Ä¢ LangChain: load_model('your-hf-repo')")                 # # LangChain chatbots


