# ðŸŽ“ THE ULTIMATE AI INTERN GUIDE: FINE-TUNING LLMS (UNSLOTH EDITION)

## 1. CORE CONCEPTS (Interview-Ready Definitions)
* **Pre-trained Model:** A "base" model (e.g., Llama-3.2) trained on general internet data. Itâ€™s smart but needs specific instructions.
* **Fine-tuning (SFT):** Supervised Fine-Tuning; teaching a model a specific task using (Instruction -> Output) pairs.
* **Quantization (4-bit):** Compressing weights from 16-bit to 4-bit. Reduces 3B model VRAM from 6GB+ to ~2.5GB.
* **LoRA (Low-Rank Adaptation):** Freezes base weights and trains two tiny matrices ($A$ and $B$).
* **Rank (r):** The width of LoRA matrices A and B. $r=16$ is the standard for performance/memory balance.
= A & B in LoRA fine-tuning: Tiny trainable matrices that multiply to fake weight changes.

Rank (r) is:
Columns in matrix A (d Ã— r)
Rows in matrix B (r Ã— k)
The shared "skinny" dimension that multiplies: B (rÃ—k) Ã— A (dÃ—r) â†’ Î”W (dÃ—k)            


Real-time:Frozen base: 
= Original weightsÂ W0W0Â stay unchanged (billions of params).
= train only A (dÃ—r) + B (rÃ—k), compute h = (Wâ‚€ + BA)x each forward

# Complete Simple Notes

## Core Idea
- Big model grid: 1000Ã—1000 = **1M numbers** to train (slow!).
- LoRA rank r=2: **A=1000Ã—2** + **B=2Ã—1000** = **4K numbers** (250x smaller!)
- Multiply **BÃ—A** fakes the big grid update Î”W

## Matrix Size Comparison
| Type | Dimensions | Numbers | Savings |
|------|------------|---------|---------|
| Full | 1000Ã—1000  | 1M      | 100%    |
| r=2  | A:1000Ã—2 + B:2Ã—1000 | 4K | 250x smaller |
| r=8  | A:2500Ã—8 + B:8Ã—2500 | 40K | 156x smaller |

## Why It Works
- **r = "key directions"** of change needed
- New tasks need only **2-64 simple patterns** (not millions)
- Train **r(d+k)** params vs **dk** params

## Real Example

* **Alpha ($\alpha$):** Scaling factor for LoRA; usually $2 \times r$ to stabilize learning.
* **Gradient Accumulation:** Summing gradients over multiple steps to simulate a large batch size on small GPUs.

---

## 2. THE FINE-TUNING HIERARCHY
| Technique            | VRAM Needed       | Trainable Params | Industry Use-Case |
| **Full Fine-Tuning** | 80GB+ (A100)      | 100%             | Large Labs / Research |
| **LoRA**             | 16GB (T4)         | ~0.1% - 1%       | Corporate Production |
| **QLoRA (Unsloth)**  | **4GB - 8GB** ðŸ”¥  | ~0.1% - 1%      | **Best for Interns/Laptops** |

---

## 3. ADVANCED TOOLING LANDSCAPE
* **Unsloth:** Optimizes Triton kernels for 2x faster training and 70% less memory.
* **HF PEFT:** The backbone library for Parameter-Efficient Fine-Tuning.
* **TRL:** Provides the `SFTTrainer` for supervised fine-tuning loops.
* **WandB:** Industry standard for experiment tracking and loss visualization.

---

## 4. COMPLETE 6-STEP PRODUCTION PIPELINE (COLAB READY)

# STEP 1: Environment Setup
!pip install --no-deps "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"
!pip install --no-deps "xformers<0.0.27" "trl<0.9.0" peft accelerate bitsandbytes

# STEP 2: Load Model with 4-bit Quantization
from unsloth import FastLanguageModel
import torch

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "unsloth/Llama-3.2-3B-Instruct",
    max_seq_length = 2048,
    load_in_4bit = True,
)

# STEP 3: Add LoRA Adapters (Targeting Attention Layers)
model = FastLanguageModel.get_peft_model(
    model,
    r = 16,
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj", 
                      "gate_proj", "up_proj", "down_proj"],
    lora_alpha = 16,
    lora_dropout = 0,
    bias = "none",
)

# STEP 4: Data Pipeline (Standardizing to ChatML)
from datasets import load_dataset
from unsloth.chat_templates import get_chat_template, standardize_sharegpt

tokenizer = get_chat_template(tokenizer, chat_template = "llama-3.2")
dataset = load_dataset("mlabonne/FineTome-100k", split = "train")
dataset = standardize_sharegpt(dataset)

def formatting_prompts_func(examples):
    texts = [tokenizer.apply_chat_template(convo, tokenize=False) for convo in examples["conversations"]]
    return {"text": texts}

dataset = dataset.map(formatting_prompts_func, batched = True)

# STEP 5: Supervised Fine-Tuning (SFT)
from trl import SFTTrainer
from transformers import TrainingArguments

trainer = SFTTrainer(
    model = model,
    tokenizer = tokenizer,
    train_dataset = dataset,
    dataset_text_field = "text",
    max_seq_length = 2048,
    args = TrainingArguments(
        per_device_train_batch_size = 2,
        gradient_accumulation_steps = 4,
        warmup_steps = 5,
        max_steps = 60,
        learning_rate = 2e-4,
        fp16 = not torch.cuda.is_bf16_supported(),
        bf16 = torch.cuda.is_bf16_supported(),
        logging_steps = 1,
        optim = "adamw_8bit",
        output_dir = "outputs",
    ),
)
trainer.train()

# STEP 6: Save LoRA Adapters for Production
model.save_pretrained("finetuned_model")
tokenizer.save_pretrained("finetuned_model")
print("âœ… Fine-tuning Complete. Model saved as 100MB adapters!")

---

## 5. MATHEMATICAL INTUITION
* **Update Rule:** $W_{new} = W_{frozen} + (A \times B)$
* **Memory Rule:** 4-bit model $\approx 0.75$ GB VRAM per 1B parameters.
* **VRAM Calc:** 3B Llama $\approx 2.4$ GB (Weights) + $2.6$ GB (Gradients/Buffers) $\approx$ **5GB total**.

---

## 6. INTERVIEW "GOLD" & TROUBLESHOOTING
* **Q: Why not Full Fine-tuning?** A: "It's too expensive and causes 'Catastrophic Forgetting'. LoRA preserves the base model while learning new tasks."
* **Q: Why Unsloth?** A: "It bypasses standard Autograd with manual backprop, saving massive memory and time."
* **Fixing OOM (Out of Memory):** Reduce `max_seq_length` or increase `gradient_accumulation_steps`.
* **Fixing Loss = NaN:** Lower the `learning_rate` to `5e-5` or check for bad data samples.

---
âœ… **GitHub Status:** Ready for Commit. AI Intern Portfolio Certified! ðŸš€
