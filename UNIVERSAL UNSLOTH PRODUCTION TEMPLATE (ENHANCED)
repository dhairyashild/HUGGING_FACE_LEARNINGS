# =============================================================================
# ðŸš€ UNIVERSAL UNSLOTH PRODUCTION TEMPLATE (ENHANCED)
# =============================================================================

# STEP 0: CONFIGURATION (Easy to modify)
MODEL_NAME = "unsloth/Llama-3.2-3B-Instruct"  # CHANGE MODEL HERE #
DATASET_PATH = "ServiceNow-AI/R1-Distill-SFT"  # CHANGE DATASET #
DATASET_CONFIG = 'v0'  # CHANGE IF NEEDED #

INPUT_COLUMN = "problem"  # CHANGE YOUR INPUT COLUMN #
OUTPUT_COLUMN = "solution"  # CHANGE YOUR OUTPUT COLUMN #

MAX_SEQ_LENGTH = 2048
LORA_RANK = 16
BATCH_SIZE = 2
MAX_STEPS = 60  # Increase for real training #

# STEP 1: LOAD MODEL WITH MEMORY OPTIMIZATION
from unsloth import FastLanguageModel
import torch

# # WHY: Auto-detects best dtype for your GPU
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = MODEL_NAME,
    max_seq_length = MAX_SEQ_LENGTH,
    dtype = None,  # Auto-selects bf16/fp16
    load_in_4bit = True,
    token = None,  # Add HF token if model is gated
)

# STEP 2: UNIVERSAL LORA SETUP
def get_target_modules(model_name):
    """Auto-selects correct LoRA modules for any model"""
    model_lower = model_name.lower()
    
    if "llama" in model_lower or "mistral" in model_lower:
        return ["q_proj", "v_proj", "k_proj", "o_proj"]  # Optimal for Llama/Mistral
    elif "phi" in model_lower:
        return ["Wqkv", "out_proj", "fc1", "fc2"]  # Phi models
    elif "gemma" in model_lower:
        return ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj"]
    else:
        return ["query", "value", "key", "dense"]  # Generic fallback

model = FastLanguageModel.get_peft_model(
    model,
    r = LORA_RANK,
    target_modules = get_target_modules(MODEL_NAME),  # # WHY: Works for any model
    lora_alpha = LORA_RANK * 2,  # # WHY: Alpha = 2*rank is standard
    lora_dropout = 0,
    use_gradient_checkpointing = "unsloth",  # # WHY: Saves 70% VRAM for long sequences
    random_state = 42,  # # WHY: Reproducible LoRA initialization
)

# STEP 3: DATA PREP WITH PROPER VALIDATION
from datasets import load_dataset
dataset = load_dataset(DATASET_PATH, DATASET_CONFIG, split="train")

# # WHY: 10% validation split prevents overfitting
dataset = dataset.train_test_split(
    test_size=0.1, 
    seed=42,  # # WHY: Fixed seed for reproducibility
    shuffle=True  # # WHY: Shuffle before splitting
)

# STEP 4: UNIVERSAL PROMPT FORMATTING
def get_prompt_template(model_name):
    """Returns appropriate prompt format for the model"""
    model_lower = model_name.lower()
    
    if "llama-3" in model_lower:
        return "<|start_header_id|>user<|end_header_id|>\n\n{instruction}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n{output}<|eot_id|>"
    elif "zephyr" in model_lower or "mistral" in model_lower:
        return "<|user|>\n{instruction}</s>\n<|assistant|>\n{output}</s>"
    elif "chatml" in model_lower:
        return "<|im_start|>user\n{instruction}<|im_end|>\n<|im_start|>assistant\n{output}<|im_end|>"
    else:
        return "### Instruction:\n{instruction}\n\n### Response:\n{output}"  # Alpaca default

def formatting_prompts_func(examples):
    """Universal formatting that works with any dataset structure"""
    # # WHY: Flexible column name handling
    inputs = examples.get(INPUT_COLUMN, [""] * len(examples[list(examples.keys())[0]]))
    outputs = examples.get(OUTPUT_COLUMN, [""] * len(inputs))
    
    template = get_prompt_template(MODEL_NAME)
    texts = []
    
    for i, o in zip(inputs, outputs):
        if i and o:  # # WHY: Skip empty examples
            formatted = template.format(instruction=i, output=o)
            texts.append(formatted)
    
    return {"text": texts}

train_ds = dataset["train"].map(formatting_prompts_func, batched=True, remove_columns=dataset["train"].column_names)
eval_ds = dataset["test"].map(formatting_prompts_func, batched=True, remove_columns=dataset["test"].column_names)

# STEP 5: PRODUCTION TRAINER WITH OPTIMIZATIONS
from trl import SFTTrainer
from transformers import TrainingArguments, EarlyStoppingCallback, DataCollatorForLanguageModeling

# # WHY: Optimizes memory usage with proper padding
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False,
    pad_to_multiple_of=8,  # # WHY: Better GPU memory alignment
)

trainer = SFTTrainer(
    model = model,
    tokenizer = tokenizer,
    train_dataset = train_ds,
    eval_dataset = eval_ds,
    dataset_text_field = "text",
    max_seq_length = MAX_SEQ_LENGTH,
    data_collator = data_collator,  # # WHY: Essential for production
    packing = False,  # Set True for short sequences to speed up training
    
    callbacks = [
        EarlyStoppingCallback(
            early_stopping_patience=3,  # # WHY: Stops if no improvement for 3 evals
            early_stopping_threshold=0.001  # # WHY: Minimum improvement required
        )
    ],
    
    args = TrainingArguments(
        per_device_train_batch_size = BATCH_SIZE,
        per_device_eval_batch_size = BATCH_SIZE,  # # WHY: Separate eval batch size
        gradient_accumulation_steps = 4,  # # WHY: Effective batch size = 2*4 = 8
        warmup_ratio = 0.1,  # # WHY: 10% of training for warmup (better than fixed steps)
        max_steps = MAX_STEPS,
        learning_rate = 2e-4,
        fp16 = not torch.cuda.is_bf16_supported(),
        bf16 = torch.cuda.is_bf16_supported(),
        logging_steps = 1,
        eval_steps = 10,
        save_steps = 20,
        evaluation_strategy = "steps",
        save_strategy = "steps",
        load_best_model_at_end = True,  # # WHY: Critical - keeps best model only
        metric_for_best_model = "eval_loss",  # # WHY: Use validation loss as metric
        greater_is_better = False,  # # WHY: Lower loss is better
        output_dir = "outputs",
        report_to = "none",  # Change to "wandb" for experiment tracking
        optim = "adamw_8bit",  # # WHY: 8-bit Adam saves memory
        lr_scheduler_type = "cosine",  # # WHY: Better learning rate decay
        seed = 42,  # # WHY: Full reproducibility
        ddp_find_unused_parameters = False,  # # WHY: Prevents DDP errors
        gradient_checkpointing = True,  # # WHY: Additional memory savings
    ),
)

# STEP 6: TRAIN & DEPLOY
print(f"ðŸš€ Training {MODEL_NAME} on {len(train_ds)} examples...")
trainer.train()

# Save everything needed for production
print("ðŸ’¾ Saving production artifacts...")

# Save best model (already loaded via load_best_model_at_end)
trainer.save_model("best_model")  # # WHY: Full model + tokenizer + config

# Save LoRA adapters separately
model.save_pretrained("lora_adapters")  # # WHY: Small file, easy to share

# Save GGUF for inference engines
model.save_pretrained_gguf(
    "model.gguf", 
    tokenizer,
    quantization_method="q4_k_m"  # # WHY: Best quality/size tradeoff
)

print("âœ… Production ready! Files saved:")
print("   - 'best_model/' â†’ Full model (load with from_pretrained)")
print("   - 'lora_adapters/' â†’ LoRA weights only")
print("   - 'model.gguf' â†’ Quantized for Ollama/LM Studio")
